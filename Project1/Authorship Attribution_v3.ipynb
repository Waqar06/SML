{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading file \n",
    "\n",
    "rawData = pd.read_csv('train_tweets.txt',sep='\\t',names=('Id','tweet'))\n",
    "\n",
    "#print(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id                                              tweet  word_count  \\\n",
      "0       8746     @handle Let's try and catch up live next week!           9   \n",
      "1       8746  Going to watch Grey's on the big screen - Thur...          11   \n",
      "2       8746  @handle My pleasure Patrick....hope you are well!           7   \n",
      "3       8746  @handle Hi there! Been traveling a lot and lot...          27   \n",
      "4       8746  RT @handle Looking to Drink Clean & Go Green? ...          19   \n",
      "5       8746  RT @handle: Ft. Hood officials confirm the 2 o...          17   \n",
      "6       8746  RT @handle: Mickey Mouse is Getting a Make Ove...          11   \n",
      "7       8746           @handle How did u get the invite Justin?           8   \n",
      "8       8746  @handle I think I am still a good friend of he...          13   \n",
      "9       8746  @handle I remember! I am fine - how are u? Wha...          12   \n",
      "10      8746     @handle That's great - good for the coach!!!!!           8   \n",
      "11      8746  @handle I don't want to picture u sitting on i...          16   \n",
      "12      8746  @handle D- Thanks for the RTs....are you going...          11   \n",
      "13      8746           @handle Grrr....you must be going crazy!           6   \n",
      "14      8746  @handle Hi there - just catching up from my tr...          15   \n",
      "15      8746  RT @handle: If you're looking for some great l...          17   \n",
      "16      8746  RT @handle: Retailers who aren’t engaging cust...          16   \n",
      "17      8746  RT @handle: Director of Global Brand Marketing...          20   \n",
      "18      8746  Still in car....want to jump out....45 minutes...           8   \n",
      "19      8746  RT @handle: \"Only surround yourself with peopl...          15   \n",
      "20      8746  @handle wish I could but 24/7 w stu's family t...          23   \n",
      "21      8746  RT @handle: Help us help MusiCares! Vote for C...          12   \n",
      "22      8746                     @handle yum!!!! Save me some!!           5   \n",
      "23      8746  RT @handle: Gratitude is the sign of noble sou...          12   \n",
      "24      8746           @handle I don't think I know what it is!           9   \n",
      "25      8746  RT @handle: @handle Just found you via @handle...          23   \n",
      "26      8746  RT @handle: RT @handle: Travelling for the Hol...          20   \n",
      "27      8746       Just entering ohio - special hi to @handle!!           8   \n",
      "28      8746  @handle well we agree on one food thing friend...          20   \n",
      "29      8746                                @handle only 1!!!!!           3   \n",
      "...      ...                                                ...         ...   \n",
      "328165  1319                                            @handle           1   \n",
      "328166  1319  @handle Please add me to the #awsms09 afterpar...           9   \n",
      "328167  1319  @handle great party last night...met some tale...           8   \n",
      "328168  1319  Alta Phoenix Lofts #1 Phoenix!!!!! Congrats to...          10   \n",
      "328169  9235                You manage things; you lead people.           6   \n",
      "328170  9235  Not to know is bad; not to wish to know is worse.          12   \n",
      "328171  9235  That there should one man die ignorant who had...          17   \n",
      "328172  9235                       Will is character in action.           5   \n",
      "328173  9235     What the mind dwells upon, the body acts upon.           9   \n",
      "328174  9235      Success as I see it, is a result, not a goal.          11   \n",
      "328175  9235  All generalizations are false, including this ...           7   \n",
      "328176  9235  It is the province of knowledge to speak and i...          17   \n",
      "328177  9235  A leader, once convinced that a particular cou...          21   \n",
      "328178  9235  You can not make excuses and money at the same...          11   \n",
      "328179  4357  Henry Brothers Electronics, Inc. to Participat...          15   \n",
      "328180  4357  TechInsights' ESC UK Event Showcases Leading C...          16   \n",
      "328181  4357  DEMOfall 09 Announces Lineup of Emerging Techn...          18   \n",
      "328182  4357  AFP Hosts Symposium on Essentials for Doing Bu...          12   \n",
      "328183  4357  AlertEnterprise Wins ASIS Accolades 2009 Secur...          11   \n",
      "328184  4357  Andrews International Introduces New Methodolo...          14   \n",
      "328185  4357  Innovation Strong Despite Recession: Human Res...          15   \n",
      "328186  4357  VideoIQ and Milestone Systems Partner to Deliv...          13   \n",
      "328187  4357  Phoenix Technologies to Showcase Cutting Edge ...          16   \n",
      "328188  4357  AnyDATA's APT-210 Tracking Device Measures Att...          12   \n",
      "328189  4357  Samplify Systems Announces Distribution Agreem...          16   \n",
      "328190  4357  Steelbox Demonstrates Open Video Framework wit...          14   \n",
      "328191  4357  Small Businesses Rely on Sage to Help Them Rid...          14   \n",
      "328192  4357  TimeSight Systems™ Announces Next-Generation P...          13   \n",
      "328193  4357  Diebold Makes Its Leading Monitoring Solutions...          11   \n",
      "328194  4357  GVI Security Solutions to Introduce AutoIP™ VM...          16   \n",
      "\n",
      "        char_count  avg_word  stopwords  hastags  numerics  upper  \n",
      "0               46  4.222222          2        0         0      0  \n",
      "1               66  5.090909          3        0         0      0  \n",
      "2               49  6.142857          2        0         0      0  \n",
      "3              132  3.925926          9        0         0      0  \n",
      "4              109  4.789474          4        0         0      1  \n",
      "5              105  5.235294          5        0         1      1  \n",
      "6               76  6.000000          2        0         0      1  \n",
      "7               40  4.125000          2        0         0      0  \n",
      "8               55  3.307692          4        0         0      2  \n",
      "9               54  3.583333          3        0         0      2  \n",
      "10              46  4.875000          2        0         0      0  \n",
      "11              77  3.875000          5        0         0      2  \n",
      "12              63  4.818182          4        0         0      2  \n",
      "13              40  5.833333          1        0         0      0  \n",
      "14              68  3.600000          7        0         0      0  \n",
      "15             105  5.235294          5        0         0      1  \n",
      "16             117  6.375000          4        0         0      1  \n",
      "17             135  5.800000          2        2         0      1  \n",
      "18              53  5.750000          2        0         0      0  \n",
      "19             101  5.800000          5        1         0      1  \n",
      "20             107  3.695652          6        0         0      1  \n",
      "21              86  6.250000          2        0         0      1  \n",
      "22              30  5.200000          1        0         0      0  \n",
      "23              74  5.250000          3        2         0      1  \n",
      "24              40  3.555556          3        0         0      2  \n",
      "25             121  4.304348          8        0         0      2  \n",
      "26             124  5.250000          6        0         0      2  \n",
      "27              44  4.625000          1        0         0      0  \n",
      "28             106  4.350000          4        1         0      1  \n",
      "29              19  5.666667          1        0         0      0  \n",
      "...            ...       ...        ...      ...       ...    ...  \n",
      "328165           7  7.000000          0        0         0      0  \n",
      "328166          56  5.333333          3        1         0      0  \n",
      "328167          57  6.250000          1        0         0      0  \n",
      "328168          79  7.000000          2        1         0      0  \n",
      "328169          35  5.000000          1        0         0      0  \n",
      "328170          49  3.166667          6        0         0      0  \n",
      "328171          93  4.529412          7        0         0      1  \n",
      "328172          28  4.800000          2        0         0      0  \n",
      "328173          46  4.222222          2        0         0      0  \n",
      "328174          45  3.181818          5        0         0      1  \n",
      "328175          50  6.285714          2        0         0      0  \n",
      "328176          85  4.058824         10        0         0      0  \n",
      "328177         125  5.000000          8        0         0      1  \n",
      "328178          52  3.818182          6        0         0      0  \n",
      "328179         118  6.933333          2        1         0      1  \n",
      "328180         135  7.500000          3        1         0      2  \n",
      "328181         134  6.500000          4        1         1      1  \n",
      "328182          92  6.750000          3        1         0      1  \n",
      "328183         101  8.272727          0        1         1      1  \n",
      "328184         135  8.714286          1        1         0      0  \n",
      "328185         135  8.066667          1        1         0      0  \n",
      "328186         122  8.461538          2        1         0      0  \n",
      "328187         134  7.437500          2        1         0      0  \n",
      "328188         108  8.083333          1        1         0      1  \n",
      "328189         134  7.437500          1        1         0      1  \n",
      "328190         132  8.500000          2        1         0      1  \n",
      "328191          96  5.928571          3        1         0      0  \n",
      "328192         129  9.000000          1        1         0      0  \n",
      "328193          99  8.090909          1        1         0      0  \n",
      "328194         128  7.062500          2        1         0      3  \n",
      "\n",
      "[328195 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Basic Feature Extraction\n",
    "\n",
    "#Number of Words\n",
    "\n",
    "rawData['word_count'] = rawData['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "rawData[['tweet','word_count']].head()\n",
    "\n",
    "#Number of characters\n",
    "\n",
    "rawData['char_count'] = rawData['tweet'].str.len() ## this also includes spaces\n",
    "rawData[['tweet','char_count']].head()\n",
    "\n",
    "\n",
    "#Average Length \n",
    "\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "rawData['avg_word'] = rawData['tweet'].apply(lambda x: avg_word(x))\n",
    "rawData[['tweet','avg_word']].head()\n",
    "\n",
    "#Number of StopWords\n",
    "\n",
    "\n",
    "\n",
    "rawData['stopwords'] = rawData['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "rawData[['tweet','stopwords']].head()\n",
    "\n",
    "#Number of Special Charaters \n",
    "\n",
    "rawData['hastags'] = rawData['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "rawData[['tweet','hastags']].head()\n",
    "\n",
    "#Numbe rof Numerics \n",
    "\n",
    "rawData['numerics'] = rawData['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "rawData[['tweet','numerics']].head()\n",
    "\n",
    "# Number of Upper Case \n",
    "\n",
    "rawData['upper'] = rawData['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "rawData[['tweet','upper']].head()\n",
    "\n",
    "#print(rawData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id                                              tweet  word_count  \\\n",
      "0       8746                                          try catch           9   \n",
      "1       8746                    grey screen thursday indulgence          11   \n",
      "2       8746                               pleasure patrickhope           7   \n",
      "3       8746   hi traveling lot lot month recovered pneumonia r          27   \n",
      "4       8746  drink clean green purchase clear2go bottle w f...          19   \n",
      "5       8746  ft hood official confirm soldier initially hel...          17   \n",
      "6       8746                       mickey mouse httpbitly1ustfu          11   \n",
      "7       8746                                      invite justin           8   \n",
      "8       8746                                                             13   \n",
      "9       8746                                remember fine whats          12   \n",
      "10      8746                                              coach           8   \n",
      "11      8746                         picture sitting understand          16   \n",
      "12      8746                                rtsare womma summit          11   \n",
      "13      8746                                 grrryou must crazy           6   \n",
      "14      8746                              hi catching trip dale          15   \n",
      "15      8746                         list mine httpbitlyjplists          17   \n",
      "16      8746  retailer arent engaging customer medium missin...          16   \n",
      "17      8746  director global brand marketing hotel casino 1...          20   \n",
      "18      8746                      carwant jump out45 minute eta           8   \n",
      "19      8746     surround lift higher oprah winfrey inspiration          15   \n",
      "20      8746    wish 247 w stus family drive sat max school sun          23   \n",
      "21      8746    musicares vote charity facebook httpbitly72cp1q          12   \n",
      "22      8746                                           yum save           5   \n",
      "23      8746    gratitude sign noble soul aesop quote gratitude          12   \n",
      "24      8746                                                              9   \n",
      "25      8746                   found sheesh sy winter break dec          23   \n",
      "26      8746  travelling holiday send pic gettington bag act...          20   \n",
      "27      8746                           entering ohio special hi           8   \n",
      "28      8746       agree food pumpkin pie grateful tweetsgiving          20   \n",
      "29      8746                                                              3   \n",
      "...      ...                                                ...         ...   \n",
      "328165  1319                                                              1   \n",
      "328166  1319                             add awsms09 afterparty           9   \n",
      "328167  1319                            party nightmet talented           8   \n",
      "328168  1319  alta phoenix loft phoenix congrats involved ht...          10   \n",
      "328169  9235                                        manage lead           6   \n",
      "328170  9235                                     bad wish worse          12   \n",
      "328171  9235       die ignorant capacity knowledge call tragedy          17   \n",
      "328172  9235                                   character action           5   \n",
      "328173  9235                     mind dwells upon body act upon           9   \n",
      "328174  9235                                success result goal          11   \n",
      "328175  9235                     generalization false including           7   \n",
      "328176  9235   province knowledge speak privilege wisdom listen          17   \n",
      "328177  9235  leader convinced particular course action must...          21   \n",
      "328178  9235                                       excuse money          11   \n",
      "328179  4357  henry brother electronics inc participate asis...          15   \n",
      "328180  4357  techinsights esc uk event showcase leading com...          16   \n",
      "328181  4357  demofall 09 announces lineup emerging technolo...          18   \n",
      "328182  4357  afp host symposium essential china httpbitly6m...          12   \n",
      "328183  4357  alertenterprise asis accolade 2009 security wi...          11   \n",
      "328184  4357  andrew international introduces methodology re...          14   \n",
      "328185  4357  innovation strong despite recession human reso...          15   \n",
      "328186  4357  videoiq milestone system partner deliver integ...          13   \n",
      "328187  4357  phoenix technology showcase cutting edge techn...          16   \n",
      "328188  4357  anydatas apt210 tracking device measure attemp...          12   \n",
      "328189  4357  samplify system announces distribution agreeme...          16   \n",
      "328190  4357  steelbox demonstrates open framework sri inter...          14   \n",
      "328191  4357  small rely sage ride recession httpbitlyfuq96 ...          14   \n",
      "328192  4357  timesight system announces nextgeneration plat...          13   \n",
      "328193  4357  diebold make leading monitoring solution avail...          11   \n",
      "328194  4357  gvi security solution introduce autoip vms ind...          16   \n",
      "\n",
      "        char_count  avg_word  stopwords  hastags  numerics  upper  \n",
      "0               46  4.222222          2        0         0      0  \n",
      "1               66  5.090909          3        0         0      0  \n",
      "2               49  6.142857          2        0         0      0  \n",
      "3              132  3.925926          9        0         0      0  \n",
      "4              109  4.789474          4        0         0      1  \n",
      "5              105  5.235294          5        0         1      1  \n",
      "6               76  6.000000          2        0         0      1  \n",
      "7               40  4.125000          2        0         0      0  \n",
      "8               55  3.307692          4        0         0      2  \n",
      "9               54  3.583333          3        0         0      2  \n",
      "10              46  4.875000          2        0         0      0  \n",
      "11              77  3.875000          5        0         0      2  \n",
      "12              63  4.818182          4        0         0      2  \n",
      "13              40  5.833333          1        0         0      0  \n",
      "14              68  3.600000          7        0         0      0  \n",
      "15             105  5.235294          5        0         0      1  \n",
      "16             117  6.375000          4        0         0      1  \n",
      "17             135  5.800000          2        2         0      1  \n",
      "18              53  5.750000          2        0         0      0  \n",
      "19             101  5.800000          5        1         0      1  \n",
      "20             107  3.695652          6        0         0      1  \n",
      "21              86  6.250000          2        0         0      1  \n",
      "22              30  5.200000          1        0         0      0  \n",
      "23              74  5.250000          3        2         0      1  \n",
      "24              40  3.555556          3        0         0      2  \n",
      "25             121  4.304348          8        0         0      2  \n",
      "26             124  5.250000          6        0         0      2  \n",
      "27              44  4.625000          1        0         0      0  \n",
      "28             106  4.350000          4        1         0      1  \n",
      "29              19  5.666667          1        0         0      0  \n",
      "...            ...       ...        ...      ...       ...    ...  \n",
      "328165           7  7.000000          0        0         0      0  \n",
      "328166          56  5.333333          3        1         0      0  \n",
      "328167          57  6.250000          1        0         0      0  \n",
      "328168          79  7.000000          2        1         0      0  \n",
      "328169          35  5.000000          1        0         0      0  \n",
      "328170          49  3.166667          6        0         0      0  \n",
      "328171          93  4.529412          7        0         0      1  \n",
      "328172          28  4.800000          2        0         0      0  \n",
      "328173          46  4.222222          2        0         0      0  \n",
      "328174          45  3.181818          5        0         0      1  \n",
      "328175          50  6.285714          2        0         0      0  \n",
      "328176          85  4.058824         10        0         0      0  \n",
      "328177         125  5.000000          8        0         0      1  \n",
      "328178          52  3.818182          6        0         0      0  \n",
      "328179         118  6.933333          2        1         0      1  \n",
      "328180         135  7.500000          3        1         0      2  \n",
      "328181         134  6.500000          4        1         1      1  \n",
      "328182          92  6.750000          3        1         0      1  \n",
      "328183         101  8.272727          0        1         1      1  \n",
      "328184         135  8.714286          1        1         0      0  \n",
      "328185         135  8.066667          1        1         0      0  \n",
      "328186         122  8.461538          2        1         0      0  \n",
      "328187         134  7.437500          2        1         0      0  \n",
      "328188         108  8.083333          1        1         0      1  \n",
      "328189         134  7.437500          1        1         0      1  \n",
      "328190         132  8.500000          2        1         0      1  \n",
      "328191          96  5.928571          3        1         0      0  \n",
      "328192         129  9.000000          1        1         0      0  \n",
      "328193          99  8.090909          1        1         0      0  \n",
      "328194         128  7.062500          2        1         0      3  \n",
      "\n",
      "[328195 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Basic Pre-processing\n",
    "\n",
    "#Lower case\n",
    "\n",
    "rawData['tweet'] = rawData['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "rawData['tweet'].head()\n",
    "\n",
    "#Punctuation removal\n",
    "\n",
    "rawData['tweet'] = rawData['tweet'].str.replace('[^\\w\\s]','')\n",
    "rawData['tweet'].head()\n",
    "\n",
    "#Stopwords removal\n",
    "\n",
    "rawData['tweet'] = rawData['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "rawData['tweet'].head()\n",
    "\n",
    "#Frequent words removal\n",
    "\n",
    "\n",
    "freq = pd.Series(' '.join(rawData['tweet']).split()).value_counts()[:10]\n",
    "    \n",
    "freq = list(freq.index)\n",
    "rawData['tweet'] = rawData['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "rawData['tweet'].head()\n",
    "\n",
    "#Rare words removal\n",
    "\n",
    "\n",
    "freq = pd.Series(' '.join(rawData['tweet']).split()).value_counts()[-10:]\n",
    "\n",
    "#Spelling correction\n",
    "\n",
    "\n",
    "freq = list(freq.index)\n",
    "rawData['tweet'] = rawData['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "rawData['tweet'].head()\n",
    "\n",
    "#Tokenization\n",
    "\n",
    "rawData['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "WordList=[]\n",
    "TextBlob(rawData['tweet'][1]).words\n",
    "WordList=['thanks', 'lyft', 'credit', 'cant', 'use', 'cause', 'dont', 'offer', 'wheelchair', 'vans', 'pdx', 'disapointed', 'getthanked']\n",
    "\n",
    "#Stemming\n",
    "\n",
    "\n",
    "st = PorterStemmer()\n",
    "rawData['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "#Lemmatization\n",
    "\n",
    "\n",
    "rawData['tweet'] = rawData['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "rawData['tweet'].head()\n",
    "\n",
    "print(rawData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262556 samples, validate on 65639 samples\n",
      "Epoch 1/30\n",
      "262556/262556 [==============================] - 99s 376us/step - loss: 8.7955 - acc: 0.0019 - val_loss: 8.6224 - val_acc: 0.0034\n",
      "Epoch 2/30\n",
      "262556/262556 [==============================] - 92s 351us/step - loss: 8.5061 - acc: 0.0038 - val_loss: 8.5556 - val_acc: 0.0046\n",
      "Epoch 3/30\n",
      "262556/262556 [==============================] - 91s 345us/step - loss: 8.4060 - acc: 0.0047 - val_loss: 8.5387 - val_acc: 0.0057\n",
      "Epoch 4/30\n",
      "262556/262556 [==============================] - 97s 368us/step - loss: 8.3338 - acc: 0.0061 - val_loss: 8.4704 - val_acc: 0.0064\n",
      "Epoch 5/30\n",
      "262556/262556 [==============================] - 100s 379us/step - loss: 8.2467 - acc: 0.0073 - val_loss: 8.4526 - val_acc: 0.0075\n",
      "Epoch 6/30\n",
      "262556/262556 [==============================] - 101s 384us/step - loss: 8.1937 - acc: 0.0080 - val_loss: 8.4219 - val_acc: 0.0082\n",
      "Epoch 7/30\n",
      "262556/262556 [==============================] - 97s 368us/step - loss: 8.1651 - acc: 0.0083 - val_loss: 8.4182 - val_acc: 0.0084\n",
      "Epoch 8/30\n",
      "169776/262556 [==================>...........] - ETA: 30s - loss: 8.1300 - acc: 0.0091"
     ]
    }
   ],
   "source": [
    "\n",
    "# ANN Libraries \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model of ANN\n",
    "\n",
    "num_dataset = rawData\n",
    "\n",
    "#print(dataset)\n",
    "#names = ['authorID','NumLink','hastags','spCount','word_count','char_count','isCapitalize','RT_count']\n",
    "\n",
    "\n",
    "X = num_dataset.iloc[:, 2:].values\n",
    "y = num_dataset.iloc[:,0].values\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "encoded_Y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_Y, test_size=0.20,random_state=1000)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=input_dim, activation='relu')) # input dimension = dimension of festure vector\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(9293, activation='softmax')) # output layer = no. of classes\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
